import subprocess

def query_llama(prompt):
    """
    Communicates with the LLaMA model using the 'ollama' command and returns the response.

    Parameters:
    prompt (str): The input prompt to send to the LLaMA model.

    Returns:
    str: The response generated by the LLaMA model.
    """
    try:
        # Run the Ollama command with the given prompt
        result = subprocess.run(['ollama', 'run', 'llama3.1'], input=prompt, capture_output=True, text=True, encoding='utf-8')
        response = result.stdout
        return response.strip()
    except Exception as e:
        return f"Error communicating with LLaMA model: {e}"

# Example usage (can be removed or commented out in production)
if __name__ == "__main__":
    prompt = "How are you?"
    print(query_llama(prompt))
